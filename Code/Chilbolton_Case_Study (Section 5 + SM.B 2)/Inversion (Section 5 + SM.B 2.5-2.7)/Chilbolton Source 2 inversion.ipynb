{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chilbolton Source 2 Inversion\n",
    "-------------------------------\n",
    "\n",
    "This notebook was used to produce the Source 2 parameter estimation results in Section 5 and Supplementary Material B 2.6. These are presented in the notebook: \"Chilbolton sources inversion results.ipynb\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>PACKAGE REQUIREMENT:</b> Package \"sourceinversion\". Install using:<br>\n",
    "pip install -q sourceinversion\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q sourceinversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>DATA:</b> Need to replace data file paths with your own local path. The files are all located in the folder:<br>\n",
    "Paper 1: Code/Data/...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing \"Sourceinversion\" Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sourceinversion.atmospheric_measurements as gp\n",
    "import sourceinversion.mcmc as mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "tfd = tfp.distributions\n",
    "import itertools\n",
    "from jax import lax\n",
    "from pyDOE import lhs\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Chilbolton Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>DATA:</b> Need to replace data file paths with your own local path. The files are all located in the folder:<br>\n",
    "Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_2/Chilbolton_CH4_measurements_source_2.pkl\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 CH4 measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Measurements\n",
      "0         2.294584\n",
      "1         2.354096\n",
      "2         2.318641\n",
      "3         2.310518\n",
      "4         2.283937\n",
      "...            ...\n",
      "2424      4.487297\n",
      "2425      5.173126\n",
      "2426      4.791353\n",
      "2427      4.924541\n",
      "2428      4.922404\n",
      "\n",
      "[2429 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('/home/newmant1/PhD/Packages/Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_2/Chilbolton_CH4_measurements_source_2.pkl', 'rb') as f:\n",
    "    observations = pickle.load(f)\n",
    "\n",
    "print(observations)\n",
    "data = observations.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Wind field and rolling standard deviation of the horizontal and vertical wind direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>DATA:</b> Need to replace data file paths with your own local path. The files are all located in the folder:<br>\n",
    "Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_2/Chilbolton_windfield_source_2.pkl\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Average Speed  Average Direction\n",
      "0         2.270231         214.082920\n",
      "1         1.722904         211.141861\n",
      "2         2.162406         215.137751\n",
      "3         2.601698         205.377329\n",
      "4         2.367214         222.392193\n",
      "..             ...                ...\n",
      "342       4.468194          69.164580\n",
      "343       4.850081          68.875514\n",
      "344       4.377566          75.887097\n",
      "345       4.493057          76.394162\n",
      "346       3.925294          68.863301\n",
      "\n",
      "[347 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('/home/newmant1/PhD/Packages/Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_2/Chilbolton_windfield_source_2.pkl', 'rb') as f:\n",
    "    tangamma_ts = pickle.load(f)\n",
    "    wind_field = tangamma_ts[['Average Speed', 'Average Direction']]\n",
    "print(wind_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Sensor layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>DATA:</b> Need to replace data file paths with your own local path. The files are all located in the folder:<br>\n",
    "Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Sensor_reflector_locations/Chilbolton_instruments_location.pkl\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/newmant1/PhD/Packages/Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Sensor_reflector_locations/Chilbolton_instruments_location.pkl', 'rb') as f:\n",
    "    instruments_location = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating integration points along beam every 0.40 meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_point_sensors = {\n",
    "    \"reflector_1\": 18*5,\n",
    "    \"reflector_2\": 33*5,\n",
    "    \"reflector_3\": 22*5,\n",
    "    \"reflector_4\": 49*5,\n",
    "    \"reflector_5\": 42*5,\n",
    "    \"reflector_6\": 29*5,\n",
    "    \"reflector_7\": 17*5,\n",
    "}\n",
    "\n",
    "def get_equally_spaced_points(point1, point2, num_points):\n",
    "    # Calculate the step size for each dimension\n",
    "    step_size = [(p2 - p1) / (num_points - 1) for p1, p2 in zip(point1, point2)]\n",
    "\n",
    "    # Calculate the coordinates of the equally spaced points\n",
    "    points = [[p1 + i * step for p1, step in zip(point1, step_size)] for i in range(num_points)]\n",
    "\n",
    "    return points\n",
    "\n",
    "point_sensors_1_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_1\"], number_of_point_sensors[\"reflector_1\"])\n",
    "point_sensors_2_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_2\"], number_of_point_sensors[\"reflector_2\"])\n",
    "point_sensors_3_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_3\"], number_of_point_sensors[\"reflector_3\"])\n",
    "point_sensors_4_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_4\"], number_of_point_sensors[\"reflector_4\"])\n",
    "point_sensors_5_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_5\"], number_of_point_sensors[\"reflector_5\"])\n",
    "point_sensors_6_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_6\"], number_of_point_sensors[\"reflector_6\"])\n",
    "point_sensors_7_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_7\"], number_of_point_sensors[\"reflector_7\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>DATA:</b> Need to replace data file paths with your own local path. The files are all located in the folder:<br>\n",
    "Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_locations_and_emission_rates/Chilbolton_sources_locations_and_emission_rates.pkl\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/newmant1/PhD/Packages/Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_locations_and_emission_rates/Chilbolton_sources_locations_and_emission_rates.pkl', 'rb') as f:\n",
    "    sources = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up for Inversion using \"sourceinversion\" package\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Variables set to \"None\" are used only when simulating gas emissions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid specification based on the Chilbolton terrain dimensions (used for Grid-based inversion)\n",
    "grid = gp.Grid(\n",
    "    x_range = (jnp.array(40.0), jnp.array(80.0)), \n",
    "    y_range = (jnp.array(0.0), jnp.array(110.0)),\n",
    "    z_range= (jnp.array(0.0), jnp.array(0.0)),\n",
    "    dx = jnp.array(10),\n",
    "    dy = jnp.array(10),\n",
    "    dz = jnp.array(1),\n",
    ")\n",
    "\n",
    "\n",
    "# Source 1 location\n",
    "source_location = gp.SourceLocation(\n",
    "    source_location_x = jnp.array([sources[\"source_2_location\"][0]]),\n",
    "    source_location_y = jnp.array([sources[\"source_2_location\"][1]]),\n",
    "    source_location_z = jnp.array([sources[\"source_2_location\"][2]]),\n",
    ")\n",
    "\n",
    "\n",
    "# Atmospheric State\n",
    "atmospheric_state = gp.AtmosphericState(\n",
    "    emission_rate = jnp.array(sources[\"source_2_emission_rate\"]),              \n",
    "    source_half_width = jnp.array(1.0),                                 # Source is a square of 2m side length\n",
    "    max_abl = jnp.array(1000.0),\n",
    "    background_mean = None,                                  \n",
    "    background_std = None,       \n",
    "    background_seed = None,\n",
    "    background_filter = None,        \n",
    "    Gaussian_filter_kernel = None,              \n",
    "    horizontal_opening_angle= None,\n",
    "    vertical_opening_angle = None,\n",
    "    a_horizontal = None,\n",
    "    a_vertical = None,          \n",
    "    b_horizontal = None,         \n",
    "    b_vertical = None,        \n",
    ")\n",
    "\n",
    "# Sensor layout\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "sensors_settings =  gp.SensorsSettings(\n",
    "    layout = None,\n",
    "    sensor_number = jnp.array(7),\n",
    "    measurement_error_var = None,\n",
    "    sensor_seed = None,\n",
    "    measurement_error_seed = None,\n",
    "    sensor_locations =  flatten_list_of_lists([point_sensors_1_location, point_sensors_2_location, point_sensors_3_location, point_sensors_4_location, point_sensors_5_location, point_sensors_6_location, point_sensors_7_location]), \n",
    ")\n",
    "\n",
    "\n",
    "# Gaussian Plume model\n",
    "gaussianplume = gp.GaussianPlume(grid, source_location, wind_field, atmospheric_state, sensors_settings)\n",
    "\n",
    "fixed  = gaussianplume.fixed_objects_of_gridfree_chilbolton_coupling_matrix(simulation = False, wind_direction=wind_field[\"Average Direction\"].values, wind_speed=wind_field[\"Average Speed\"].values, tangamma_ts = tangamma_ts, number_of_time_steps=wind_field.shape[0])\n",
    "fixed_ref1 = fixed[0], fixed[7], fixed[14], fixed[15], fixed[35], fixed[36], fixed[16], fixed[37], fixed[44]\n",
    "fixed_ref2 = fixed[1], fixed[8], fixed[17], fixed[18], fixed[35], fixed[36], fixed[19], fixed[38], fixed[45]\n",
    "fixed_ref3 = fixed[2], fixed[9], fixed[20], fixed[21], fixed[35], fixed[36], fixed[22], fixed[39], fixed[46]\n",
    "fixed_ref4 = fixed[3], fixed[10], fixed[23], fixed[24], fixed[35], fixed[36], fixed[25], fixed[40], fixed[47]\n",
    "fixed_ref5 = fixed[4], fixed[11], fixed[26], fixed[27], fixed[35], fixed[36], fixed[28], fixed[41], fixed[48]\n",
    "fixed_ref6 = fixed[5], fixed[12], fixed[29], fixed[30], fixed[35], fixed[36], fixed[31], fixed[42], fixed[49]\n",
    "fixed_ref7 = fixed[6], fixed[13], fixed[32], fixed[33], fixed[35], fixed[36], fixed[34], fixed[43], fixed[50]\n",
    "\n",
    "\n",
    "# Parameter priors\n",
    "priors = mcmc.Priors(\n",
    "    # Slab allocation rate prior (used in grid-based inversion)\n",
    "    theta = 0.1,\n",
    "\n",
    "    # Emission rate (log(s)): Log scale Slab and spike prior (used in grid-based inversion)\n",
    "    log_spike_mean = -25.0,\n",
    "    log_spike_var = 10.0,\n",
    "    log_slab_mean = -7.5,\n",
    "    log_slab_var = 1.5,\n",
    "\n",
    "    # Source location (x,y):\n",
    "    source_location_x_mean = 50.0,\n",
    "    source_location_x_var = 25.0,\n",
    "    source_location_y_mean = 50.0,\n",
    "    source_location_y_var = 25.0,\n",
    "\n",
    "    # Measurement error variance (sigma squared)\n",
    "    sigma_squared_con = 1e-11,\n",
    "    sigma_squared_rate = 1e-8,\n",
    "\n",
    "    # Background gas concentration (beta)\n",
    "    mean_background_prior = 1.92,\n",
    "    variance_background_prior = 0.1**2,\n",
    "\n",
    "    # Dispersion parameter (a_H, a_V, b_H, b_V)\n",
    "    a_mean = jnp.log(0.6),\n",
    "    a_var = 0.5**2,\n",
    "    b_mean = jnp.log(0.6),\n",
    "    b_var = 0.2**2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Source 1 Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Grid Search\n",
    "\n",
    "Here we only estimate source emission rate and location while fixing background gas concentration, measurement error variance and dispersion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.    , 0.0002, 0.0004, 0.0006, 0.0008]),\n",
       " array([ 0, 10, 20, 30, 40, 50, 60, 70]),\n",
       " array([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_granulaty_in_kg_per_s = 0.0002\n",
    "\n",
    "s_range = np.arange(0, 0.001, emission_granulaty_in_kg_per_s)\n",
    "x_range = grid.x\n",
    "y_range = grid.y\n",
    "\n",
    "ranges = []\n",
    "ranges.append(s_range)\n",
    "ranges.append(x_range)\n",
    "ranges.append(y_range)\n",
    "\n",
    "ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_likelihood(s, x, y, initial_sgm, initial_betas, scheme, stability_class, number_time_steps):\n",
    "    \"\"\"\n",
    "    Returns the positive log posterior of the point sensors measurements model. \n",
    "\n",
    "    \"\"\"\n",
    "    if stability_class == False:\n",
    "        stability_class = None\n",
    "        estimated = True\n",
    "    else:\n",
    "        estimated = False\n",
    "        \n",
    "    coupling_matrix_ref1 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref1, x, y, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref2 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref2, x, y, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref3 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref3, x, y, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref4 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref4, x, y, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref5 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref5, x, y, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref6 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref6, x, y, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref7 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref7, x, y, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    \n",
    "\n",
    "    reshaped_coupling_matrix_ref1 = coupling_matrix_ref1.reshape(number_time_steps,18*5, order='F')\n",
    "    reshaped_coupling_matrix_ref2 = coupling_matrix_ref2.reshape(number_time_steps,33*5, order='F')\n",
    "    reshaped_coupling_matrix_ref3 = coupling_matrix_ref3.reshape(number_time_steps,22*5, order='F')\n",
    "    reshaped_coupling_matrix_ref4 = coupling_matrix_ref4.reshape(number_time_steps,49*5, order='F')\n",
    "    reshaped_coupling_matrix_ref5 = coupling_matrix_ref5.reshape(number_time_steps,42*5, order='F')\n",
    "    reshaped_coupling_matrix_ref6 = coupling_matrix_ref6.reshape(number_time_steps,29*5, order='F')\n",
    "    reshaped_coupling_matrix_ref7 = coupling_matrix_ref7.reshape(number_time_steps,17*5, order='F')\n",
    "\n",
    "    path_averaged_coupling_matrix_ref1 = reshaped_coupling_matrix_ref1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref2 = reshaped_coupling_matrix_ref2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref3 = reshaped_coupling_matrix_ref3.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref4 = reshaped_coupling_matrix_ref4.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref5 = reshaped_coupling_matrix_ref5.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref6 = reshaped_coupling_matrix_ref6.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref7 = reshaped_coupling_matrix_ref7.mean(axis=1)\n",
    "\n",
    "    path_averaged_A = [path_averaged_coupling_matrix_ref1, path_averaged_coupling_matrix_ref2, path_averaged_coupling_matrix_ref3, path_averaged_coupling_matrix_ref4, path_averaged_coupling_matrix_ref5, path_averaged_coupling_matrix_ref6, path_averaged_coupling_matrix_ref7]\n",
    "    A = jnp.array(path_averaged_A).reshape(-1,1)\n",
    "\n",
    "    log_likelihood = tfd.Normal(loc = (jnp.matmul(A,s.reshape(-1,1)) + jnp.repeat(initial_betas, number_time_steps).reshape(-1,1)), \\\n",
    "                                scale= jnp.sqrt(initial_sgm)).log_prob(data)\n",
    "\n",
    "    log_posterior = jnp.sum(log_likelihood) \n",
    "\n",
    "    return log_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter combinations: 440\n"
     ]
    }
   ],
   "source": [
    "# Generate all combinations of parameters\n",
    "sources = jnp.zeros(len(ranges))\n",
    "parameter_combinations = jnp.array(list(itertools.product(*ranges[:len(ranges)+1])))\n",
    "print(f\"Number of parameter combinations: {parameter_combinations.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search maximum likelihood estimation of emission rate and location:\n",
      "          s     x     y\n",
      "335  0.0006  60.0  50.0\n"
     ]
    }
   ],
   "source": [
    "# Initial parameter values for:\n",
    "\n",
    "# Sensor measurement error variance\n",
    "initial_sgm = 1e-5\n",
    "# Background gas concentration\n",
    "initial_betas = jnp.repeat(1.92, 7)\n",
    "\n",
    "# Step function\n",
    "def one_step(_, parameters):\n",
    "    s, x, y= parameters[::3], parameters[1::3], parameters[2::3]\n",
    "    new_likelihood = search_likelihood(s,x,y, initial_sgm, initial_betas, \"Draxler\", False, wind_field.shape[0]) \n",
    "    return new_likelihood, new_likelihood\n",
    "# Use lax.scan to iterate over the parameter combinations\n",
    "_, likelihoods = lax.scan(one_step, 0.0, parameter_combinations)\n",
    "# Analysing output\n",
    "likelihood_df = pd.DataFrame(likelihoods, columns = ['log_likelihood'])\n",
    "parameters_df = pd.DataFrame(parameter_combinations, columns = [\"s\", \"x\", \"y\"])\n",
    "max_likelihood_index = likelihood_df.idxmax()\n",
    "max_likelihood_row = parameters_df.iloc[max_likelihood_index]\n",
    "sources = sources.at[:].set(max_likelihood_row.values.flatten())\n",
    "print(f\"Grid search maximum likelihood estimation of emission rate and location:\")\n",
    "print(max_likelihood_row)\n",
    "\n",
    "rates = max_likelihood_row.values.flatten()[0]\n",
    "loc_x = max_likelihood_row.values.flatten()[1]\n",
    "loc_y = max_likelihood_row.values.flatten()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Latin Hypercube\n",
    "\n",
    "Here we estimate all parameters simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latin Hypercube maximum likelihood estimation:\n",
      "          s_1        x_1        y_1       sgm       a_h       a_v       b_h  \\\n",
      "825  0.000596  58.303562  48.258161  0.000091  0.912349  1.152563  1.001251   \n",
      "\n",
      "          b_v  \n",
      "825  0.769914  \n"
     ]
    }
   ],
   "source": [
    "def latin_hypercube_sampling(param_ranges, num_samples):\n",
    "    \"\"\"\n",
    "    Generate a Latin Hypercube Sample within specified parameter ranges.\n",
    "\n",
    "    Parameters:\n",
    "    param_ranges (list of tuple): A list of tuples specifying the range (min, max) for each parameter.\n",
    "    num_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A num_samples x num_params matrix where each column corresponds to a parameter and each row corresponds to a sample.\n",
    "    \"\"\"\n",
    "    num_params = len(param_ranges)\n",
    "    \n",
    "    # Generate the Latin Hypercube Sample\n",
    "    lhd = lhs(num_params, num_samples)\n",
    "    \n",
    "    # Scale the samples to be within the parameter ranges\n",
    "    for i in range(num_params):\n",
    "        min_val, max_val = param_ranges[i]\n",
    "        lhd[:, i] = lhd[:, i] * (max_val - min_val) + min_val\n",
    "\n",
    "    return lhd\n",
    "\n",
    "\n",
    "def latin_hypercube_likelihood(s, x, y, sgm, a_h, a_v, b_h, b_v, scheme, stability_class, number_time_steps):\n",
    "    \"\"\"\n",
    "    Returns the positive log posterior of the point sensors measurements model. \n",
    "\n",
    "    \"\"\"\n",
    "    if stability_class == False:\n",
    "        stability_class = None\n",
    "        estimated = True\n",
    "    else:\n",
    "        estimated = False\n",
    "\n",
    "    coupling_matrix_ref1 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref1, x, y, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation = False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref2 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref2, x, y, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation = False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref3 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref3, x, y, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation = False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref4 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref4, x, y, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation = False, estimated=estimated, scheme=scheme, stability_class=stability_class)    \n",
    "    coupling_matrix_ref5 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref5, x, y, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation = False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref6 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref6, x, y, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation = False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref7 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref7, x, y, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation = False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    \n",
    "\n",
    "    reshaped_coupling_matrix_ref1 = coupling_matrix_ref1.reshape(number_time_steps,18*5, order='F')\n",
    "    reshaped_coupling_matrix_ref2 = coupling_matrix_ref2.reshape(number_time_steps,33*5, order='F')\n",
    "    reshaped_coupling_matrix_ref3 = coupling_matrix_ref3.reshape(number_time_steps,22*5, order='F')\n",
    "    reshaped_coupling_matrix_ref4 = coupling_matrix_ref4.reshape(number_time_steps,49*5, order='F')\n",
    "    reshaped_coupling_matrix_ref5 = coupling_matrix_ref5.reshape(number_time_steps,42*5, order='F')\n",
    "    reshaped_coupling_matrix_ref6 = coupling_matrix_ref6.reshape(number_time_steps,29*5, order='F')\n",
    "    reshaped_coupling_matrix_ref7 = coupling_matrix_ref7.reshape(number_time_steps,17*5, order='F')\n",
    "\n",
    "    path_averaged_coupling_matrix_ref1 = reshaped_coupling_matrix_ref1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref2 = reshaped_coupling_matrix_ref2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref3 = reshaped_coupling_matrix_ref3.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref4 = reshaped_coupling_matrix_ref4.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref5 = reshaped_coupling_matrix_ref5.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref6 = reshaped_coupling_matrix_ref6.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref7 = reshaped_coupling_matrix_ref7.mean(axis=1)\n",
    "\n",
    "    path_averaged_A = [path_averaged_coupling_matrix_ref1, path_averaged_coupling_matrix_ref2, path_averaged_coupling_matrix_ref3, path_averaged_coupling_matrix_ref4, path_averaged_coupling_matrix_ref5, path_averaged_coupling_matrix_ref6, path_averaged_coupling_matrix_ref7]\n",
    "    A = jnp.array(path_averaged_A).reshape(-1,1)\n",
    "\n",
    "    log_likelihood = tfd.Normal(loc = (jnp.matmul(A, s.reshape(-1,1)) + jnp.repeat(initial_betas, number_time_steps).reshape(-1,1)), \\\n",
    "                                scale= jnp.sqrt(sgm)).log_prob(data) \n",
    "    \n",
    "    log_posterior = jnp.sum(log_likelihood)\n",
    "\n",
    "    return log_posterior\n",
    "\n",
    "param_ranges = []\n",
    "param_ranges.append((np.maximum(0, rates - emission_granulaty_in_kg_per_s), rates + emission_granulaty_in_kg_per_s))\n",
    "param_ranges.append((loc_x - grid.dx, loc_x + grid.dx))\n",
    "param_ranges.append((loc_y - grid.dy, loc_y + grid.dy))\n",
    "\n",
    "param_ranges.append((0, 1e-4))\n",
    "param_ranges.append((0.5, 1.2))\n",
    "param_ranges.append((0.5, 1.2))\n",
    "param_ranges.append((0.5, 1.01))\n",
    "param_ranges.append((0.5, 1.01))\n",
    "\n",
    "\n",
    "num_samples = 1_000\n",
    "lh_samples = latin_hypercube_sampling(param_ranges, num_samples)\n",
    "# Step function\n",
    "def lh_one_step(_, parameters):\n",
    "    rates = jnp.zeros(int(len(ranges)/3))\n",
    "    loc_x = jnp.zeros(int(len(ranges)/3))\n",
    "    loc_y = jnp.zeros(int(len(ranges)/3))\n",
    "    for source_nbr in range(int(len(ranges)/3)):\n",
    "        rates = rates.at[source_nbr].set(parameters[source_nbr*3])\n",
    "        loc_x = loc_x.at[source_nbr].set(parameters[source_nbr*3+1])\n",
    "        loc_y = loc_y.at[source_nbr].set(parameters[source_nbr*3+2])\n",
    "    sgm, a_h, a_v, b_h, b_v = parameters[int(len(ranges)):]\n",
    "    new_likelihood = latin_hypercube_likelihood(rates, loc_x, loc_y, sgm, a_h, a_v, b_h, b_v, \"Draxler\", False, wind_field.shape[0])\n",
    "    return new_likelihood, new_likelihood\n",
    "# Use lax.scan to iterate over the parameter combinations\n",
    "_, lh_likelihoods = lax.scan(lh_one_step, 0.0, lh_samples)\n",
    "\n",
    "# Analysing output\n",
    "lh_likelihood_df = pd.DataFrame(lh_likelihoods, columns = ['log_likelihood'])\n",
    "lh_parameters_df = pd.DataFrame(lh_samples, columns = list(itertools.chain(*[['s_' + str(i+1), 'x_' + str(i+1), 'y_' + str(i+1)] for i in range(int(len(ranges)/3))])) + ['sgm', 'a_h', 'a_v', 'b_h', 'b_v'])\n",
    "lh_max_likelihood_index = lh_likelihood_df.idxmax()\n",
    "lh_max_likelihood_row = lh_parameters_df.iloc[lh_max_likelihood_index]\n",
    "print(f\"Latin Hypercube maximum likelihood estimation:\")\n",
    "print(lh_max_likelihood_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Manifold-Metropolis-within-Gibbs\n",
    "\n",
    "Here we estimate all parameters simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Initial Parameter Values:\n",
      "--------------------------------\n",
      "Initial log rates: [-7.42445432]\n",
      "Initial locations x: [58.30356172]\n",
      "Initial locations y: [48.25816124]\n",
      "Initial sgm sqr: 9.143420939442626e-05\n",
      "Initial log a_h: -0.09173245956518437\n",
      "Initial log a_v: 0.14198814307303614\n",
      "Initial log b_h: 0.001249826627458649\n",
      "Initial log b_v: -0.2614766030300394\n"
     ]
    }
   ],
   "source": [
    "initial_log_rates = jnp.log(lh_max_likelihood_row[['s_' + str(i+1) for i in range(int(len(ranges)/3))]].values.flatten())\n",
    "initial_locations_x = lh_max_likelihood_row[['x_' + str(i+1) for i in range(int(len(ranges)/3))]].values.flatten()\n",
    "initial_locations_y = lh_max_likelihood_row[['y_' + str(i+1) for i in range(int(len(ranges)/3))]].values.flatten()\n",
    "initial_sgm_sqr = lh_max_likelihood_row.values.flatten()[3]\n",
    "initial_log_a_h = jnp.log(lh_max_likelihood_row.values.flatten()[4])\n",
    "initial_log_a_v = jnp.log(lh_max_likelihood_row.values.flatten()[5])\n",
    "initial_log_b_h = jnp.log(lh_max_likelihood_row.values.flatten()[6])\n",
    "initial_log_b_v = jnp.log(lh_max_likelihood_row.values.flatten()[7])\n",
    "\n",
    "print(f\"Setting Initial Parameter Values:\")\n",
    "print(f\"--------------------------------\")\n",
    "print(f\"Initial log rates: {initial_log_rates}\")\n",
    "print(f\"Initial locations x: {initial_locations_x}\")\n",
    "print(f\"Initial locations y: {initial_locations_y}\")\n",
    "print(f\"Initial sgm sqr: {initial_sgm_sqr}\")\n",
    "print(f\"Initial log a_h: {initial_log_a_h}\")\n",
    "print(f\"Initial log a_v: {initial_log_a_v}\")\n",
    "print(f\"Initial log b_h: {initial_log_b_h}\")\n",
    "print(f\"Initial log b_v: {initial_log_b_v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gibbsparams = {\n",
    "    'background': initial_betas,\n",
    "    'sigma_squared':   initial_sgm_sqr,\n",
    "}\n",
    "gibbs_flat, gibbs_unflat_func = ravel_pytree(Gibbsparams)\n",
    "\n",
    "MHparams = {\n",
    "    'log_a_H': initial_log_a_h,\n",
    "    'log_a_V': initial_log_a_v,\n",
    "    'log_b_H':  initial_log_b_h,\n",
    "    'log_b_V': initial_log_b_v,\n",
    "    'log_s': jnp.array(initial_log_rates),\n",
    "    'source_x': jnp.array(initial_locations_x),\n",
    "    'source_y': jnp.array(initial_locations_y),\n",
    "    }\n",
    "mh_flat, mh_unflat_func = ravel_pytree(MHparams)\n",
    "\n",
    "\n",
    "def log_posterior(params, sigma_squared, betas, ss_var, ss_mean, data, priors, wind_sigma, number_of_time_steps):\n",
    "    \"\"\"\n",
    "    Returns the positive log posterior of the point sensors measurements model. \n",
    "\n",
    "    \"\"\"\n",
    "    if wind_sigma == True:\n",
    "        coupling_matrix_ref1 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref1, params[\"source_x\"], params[\"source_y\"], jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref2 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref2, params[\"source_x\"], params[\"source_y\"], jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref3 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref3, params[\"source_x\"], params[\"source_y\"], jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref4 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref4, params[\"source_x\"], params[\"source_y\"], jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref5 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref5, params[\"source_x\"], params[\"source_y\"], jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref6 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref6, params[\"source_x\"], params[\"source_y\"], jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref7 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref7, params[\"source_x\"], params[\"source_y\"], jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "    elif wind_sigma == False:\n",
    "        coupling_matrix_ref1 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref1,params[\"source_x\"],params[\"source_y\"], False, False, False, False, simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref2 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref2,params[\"source_x\"],params[\"source_y\"], False, False, False, False, simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref3 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref3,params[\"source_x\"],params[\"source_y\"], False, False, False, False, simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref4 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref4,params[\"source_x\"],params[\"source_y\"], False, False, False, False, simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref5 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref5,params[\"source_x\"],params[\"source_y\"], False, False, False, False, simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref6 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref6,params[\"source_x\"],params[\"source_y\"], False, False, False, False, simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref7 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref7,params[\"source_x\"],params[\"source_y\"], False, False, False, False, simulation = False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "\n",
    "    reshaped_coupling_matrix_ref1 = coupling_matrix_ref1.reshape(number_of_time_steps,18*5, order='F')\n",
    "    reshaped_coupling_matrix_ref2 = coupling_matrix_ref2.reshape(number_of_time_steps,33*5, order='F')\n",
    "    reshaped_coupling_matrix_ref3 = coupling_matrix_ref3.reshape(number_of_time_steps,22*5, order='F')\n",
    "    reshaped_coupling_matrix_ref4 = coupling_matrix_ref4.reshape(number_of_time_steps,49*5, order='F')\n",
    "    reshaped_coupling_matrix_ref5 = coupling_matrix_ref5.reshape(number_of_time_steps,42*5, order='F')\n",
    "    reshaped_coupling_matrix_ref6 = coupling_matrix_ref6.reshape(number_of_time_steps,29*5, order='F')\n",
    "    reshaped_coupling_matrix_ref7 = coupling_matrix_ref7.reshape(number_of_time_steps,17*5, order='F')\n",
    "\n",
    "    path_averaged_coupling_matrix_ref1 = reshaped_coupling_matrix_ref1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref2 = reshaped_coupling_matrix_ref2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref3 = reshaped_coupling_matrix_ref3.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref4 = reshaped_coupling_matrix_ref4.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref5 = reshaped_coupling_matrix_ref5.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref6 = reshaped_coupling_matrix_ref6.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref7 = reshaped_coupling_matrix_ref7.mean(axis=1)\n",
    "\n",
    "    path_averaged_A = [path_averaged_coupling_matrix_ref1, path_averaged_coupling_matrix_ref2, path_averaged_coupling_matrix_ref3, path_averaged_coupling_matrix_ref4, path_averaged_coupling_matrix_ref5, path_averaged_coupling_matrix_ref6, path_averaged_coupling_matrix_ref7]\n",
    "    A = jnp.array(path_averaged_A).reshape(-1,1)\n",
    "\n",
    "    log_likelihood = tfd.Normal(loc = (jnp.matmul(A,jnp.exp(params[\"log_s\"]).reshape(-1,1))+ betas), \\\n",
    "                                scale= jnp.sqrt(sigma_squared)).log_prob(data)\n",
    "\n",
    "    if wind_sigma == True:\n",
    "        log_prior_a_H = tfd.Normal(loc = priors.a_mean, scale = jnp.sqrt(priors.a_var)).log_prob(params[\"log_a_H\"])\n",
    "        log_prior_a_V = tfd.Normal(loc = priors.a_mean, scale = jnp.sqrt(priors.a_var)).log_prob(params[\"log_a_V\"])\n",
    "        log_prior_b_H = tfd.Normal(loc = priors.b_mean, scale = jnp.sqrt(priors.b_var)).log_prob(params[\"log_b_H\"])\n",
    "        log_prior_b_V = tfd.Normal(loc = priors.b_mean, scale = jnp.sqrt(priors.b_var)).log_prob(params[\"log_b_V\"])\n",
    "\n",
    "    log_posterior_emission_rate = tfd.MultivariateNormalDiag(loc = ss_mean, scale_diag = jnp.sqrt(ss_var)).log_prob(params[\"log_s\"].reshape(-1,1))\n",
    "    log_posterior_source_location = tfd.MultivariateNormalDiag(loc = jnp.array([priors.source_location_x_mean, priors.source_location_y_mean]), \\\n",
    "                                                                scale_diag = jnp.sqrt(jnp.array([priors.source_location_x_var, priors.source_location_y_var]))).log_prob(jnp.array([params[\"source_x\"], params[\"source_y\"]]).flatten())\n",
    "\n",
    "    if wind_sigma == True:\n",
    "        log_posterior = jnp.sum(log_likelihood) + jnp.sum(log_posterior_source_location) + jnp.sum(log_posterior_emission_rate) + jnp.sum(log_prior_a_H) + jnp.sum(log_prior_a_V) + jnp.sum(log_prior_b_H) + jnp.sum(log_prior_b_V) \n",
    "    elif wind_sigma == False:\n",
    "        log_posterior = jnp.sum(log_likelihood) + jnp.sum(log_posterior_source_location) + jnp.sum(log_posterior_emission_rate)\n",
    "\n",
    "    return log_posterior, A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10_000\n",
    "initial_step_size = 0.5\n",
    "# Run the MCMC algorithm\n",
    "mala_chains = mcmc.Manifold_MALA_Within_Gibbs(False, gaussianplume, data, log_posterior, priors, MHparams, Gibbsparams, fixed, chilbolton=True, wind_sigmas=True, release_17 = False, step_size_tuning=\"False\").manifold_mala_chains(Gibbsparams, mh_flat, iterations, initial_step_size, release_17=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
