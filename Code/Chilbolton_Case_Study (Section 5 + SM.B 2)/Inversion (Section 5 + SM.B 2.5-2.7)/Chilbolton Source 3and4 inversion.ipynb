{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chilbolton Source 3 and Source 4 Inversion\n",
    "-------------------------------\n",
    "\n",
    "This notebook was used to produce the Source 3 and Source 4 parameter estimation results in Section 5 and Supplementary Material B 2.7. These are presented in the notebook: \"Chilbolton sources inversion results.ipynb\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>PACKAGE REQUIREMENT:</b> Package \"sourceinversion\". Install using:<br>\n",
    "pip install -q sourceinversion\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q sourceinversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>DATA:</b> Need to replace data file paths with your own local path. The files are all located in the folder:<br>\n",
    "Paper 1: Code/Data/...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing \"sourceinversion\" Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 13:27:51.429565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sourceinversion.atmospheric_measurements as gp\n",
    "import sourceinversion.mcmc as mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "tfd = tfp.distributions\n",
    "import itertools\n",
    "from jax import lax\n",
    "from pyDOE import lhs\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Chilbolton Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>DATA:</b> Need to replace data file paths with your own local path. The files are all located in the folder:<br>\n",
    "Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_3_and_4/Chilbolton_CH4_measurements_source_3and4.pkl\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 CH4 measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Measurements\n",
      "0         2.156402\n",
      "1         2.323252\n",
      "2         2.695396\n",
      "3         2.180471\n",
      "4         2.181575\n",
      "...            ...\n",
      "1644      2.077906\n",
      "1645      2.079868\n",
      "1646      2.114129\n",
      "1647      2.995524\n",
      "1648      2.982547\n",
      "\n",
      "[1649 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('/home/newmant1/PhD/Packages/Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_3_and_4/Chilbolton_CH4_measurements_source_3and4.pkl', 'rb') as f:\n",
    "    observations = pickle.load(f)\n",
    "\n",
    "print(observations)\n",
    "data = observations.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Wind field and rolling standard deviation of the horizontal and vertical wind direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>DATA:</b> Need to replace data file paths with your own local path. The files are all located in the folder:<br>\n",
    "Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_3_and_4/Chilbolton_windfield_source_3and4.pkl\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Average Speed  Average Direction\n",
      "0         2.148621         161.316738\n",
      "1         2.742933         147.034253\n",
      "2         2.670685         152.610808\n",
      "3         2.793845         166.890524\n",
      "4         3.088895         158.469113\n",
      "..             ...                ...\n",
      "235       3.073157          66.605103\n",
      "236       3.270097          69.169916\n",
      "237       4.056264          77.726880\n",
      "238       4.371059          82.811326\n",
      "239       5.100623          85.347159\n",
      "\n",
      "[240 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('/home/newmant1/PhD/Packages/Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_3_and_4/Chilbolton_windfield_source_3and4.pkl', 'rb') as f:\n",
    "    tangamma_ts = pickle.load(f)\n",
    "    wind_field = tangamma_ts[['Average Speed', 'Average Direction']]\n",
    "    \n",
    "print(wind_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing release_17 data: \n",
    "The inversion can be made with release_17 data or without (do not run the code below if wishing to include release_17). All release_17 beam 2 observations are corrupt and therefore were removed during the data processing. Release_17 therefore contains measurements for all beams apart from beam 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Average Speed  Average Direction\n",
      "0         2.148621         161.316738\n",
      "1         2.742933         147.034253\n",
      "2         2.670685         152.610808\n",
      "3         2.793845         166.890524\n",
      "4         3.088895         158.469113\n",
      "..             ...                ...\n",
      "204       3.853372         125.586786\n",
      "205       4.038335         124.080517\n",
      "206       4.336893         128.099723\n",
      "207       4.898623         146.980776\n",
      "208       4.857069         148.027751\n",
      "\n",
      "[209 rows x 2 columns]\n",
      "      Measurements\n",
      "0         2.156402\n",
      "1         2.323252\n",
      "2         2.695396\n",
      "3         2.180471\n",
      "4         2.181575\n",
      "...            ...\n",
      "1644      2.077906\n",
      "1645      2.079868\n",
      "1646      2.114129\n",
      "1647      2.995524\n",
      "1648      2.982547\n",
      "\n",
      "[1463 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "wind_field = wind_field.iloc[0:209]\n",
    "print(wind_field)\n",
    "\n",
    "# Create a grouping key\n",
    "group_key = observations.index // 240\n",
    "\n",
    "# Select the first 209 (=240-31) rows from each group\n",
    "df_new = observations.groupby(group_key).head(209)\n",
    "observations = df_new\n",
    "\n",
    "print(observations)\n",
    "data = observations.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Sensor layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>DATA:</b> Need to replace data file paths with your own local path. The files are all located in the folder:<br>\n",
    "Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Sensor_reflector_locations/Chilbolton_instruments_location.pkl\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/newmant1/PhD/Packages/Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Sensor_reflector_locations/Chilbolton_instruments_location.pkl', 'rb') as f:\n",
    "    instruments_location = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating integration points along beam every 0.40 meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_point_sensors = {\n",
    "    \"reflector_1\": 18*5,\n",
    "    \"reflector_2\": 33*5,\n",
    "    \"reflector_3\": 22*5,\n",
    "    \"reflector_4\": 49*5,\n",
    "    \"reflector_5\": 42*5,\n",
    "    \"reflector_6\": 29*5,\n",
    "    \"reflector_7\": 17*5,\n",
    "}\n",
    "\n",
    "def get_equally_spaced_points(point1, point2, num_points):\n",
    "    # Calculate the step size for each dimension\n",
    "    step_size = [(p2 - p1) / (num_points - 1) for p1, p2 in zip(point1, point2)]\n",
    "\n",
    "    # Calculate the coordinates of the equally spaced points\n",
    "    points = [[p1 + i * step for p1, step in zip(point1, step_size)] for i in range(num_points)]\n",
    "\n",
    "    return points\n",
    "\n",
    "point_sensors_1_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_1\"], number_of_point_sensors[\"reflector_1\"])\n",
    "point_sensors_2_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_2\"], number_of_point_sensors[\"reflector_2\"])\n",
    "point_sensors_3_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_3\"], number_of_point_sensors[\"reflector_3\"])\n",
    "point_sensors_4_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_4\"], number_of_point_sensors[\"reflector_4\"])\n",
    "point_sensors_5_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_5\"], number_of_point_sensors[\"reflector_5\"])\n",
    "point_sensors_6_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_6\"], number_of_point_sensors[\"reflector_6\"])\n",
    "point_sensors_7_location = get_equally_spaced_points(instruments_location[\"line_of_sight_sensor\"], instruments_location[\"reflector_7\"], number_of_point_sensors[\"reflector_7\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>DATA:</b> Need to replace data file paths with your own local path. The files are all located in the folder:<br>\n",
    "Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_locations_and_emission_rates/Chilbolton_sources_locations_and_emission_rates.pkl\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/newmant1/PhD/Packages/Paper 1: Code/Data/Chilbolton_data_files/Postprocessed/Source_locations_and_emission_rates/Chilbolton_sources_locations_and_emission_rates.pkl', 'rb') as f:\n",
    "    sources = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up for Inversion using \"sourceinversion\" package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Variables set to \"None\" are used only when simulating gas emissions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# Grid specification based on the Chilbolton terrain dimensions\n",
    "# (used grid Search parameter initialisation or grid-based inversion)\n",
    "grid = gp.Grid(\n",
    "    x_range = (jnp.array(40.0), jnp.array(80.0)), \n",
    "    y_range = (jnp.array(0.0), jnp.array(110.0)),\n",
    "    z_range= (jnp.array(0.0), jnp.array(0.0)),\n",
    "    dx = jnp.array(10),\n",
    "    dy = jnp.array(10),\n",
    "    dz = jnp.array(1),\n",
    ")\n",
    "\n",
    "\n",
    "# Source 3 and Source 4 location\n",
    "source_location = gp.SourceLocation(\n",
    "    source_location_x = jnp.array([sources[\"source_3_location\"][0], sources[\"source_4_location\"][0]]),\n",
    "    source_location_y = jnp.array([sources[\"source_3_location\"][1], sources[\"source_4_location\"][1]]),\n",
    "    source_location_z = jnp.array([sources[\"source_3_location\"][2], sources[\"source_4_location\"][2]]),\n",
    ")\n",
    "\n",
    "\n",
    "# Atmospheric State\n",
    "atmospheric_state = gp.AtmosphericState(\n",
    "    emission_rate = jnp.array([sources[\"source_3_emission_rate\"], \\\n",
    "                            sources[\"source_4_emission_rate\"]]),              \n",
    "    source_half_width = jnp.array(1.0),                                 # Source is a square of 2m side length\n",
    "    max_abl = jnp.array(1000.0),\n",
    "    background_mean = None,                                  \n",
    "    background_std = None,       \n",
    "    background_seed = None,\n",
    "    background_filter = None,        \n",
    "    Gaussian_filter_kernel = None,              \n",
    "    horizontal_opening_angle= None,\n",
    "    vertical_opening_angle = None,\n",
    "    a_horizontal = None,\n",
    "    a_vertical = None,          \n",
    "    b_horizontal = None,         \n",
    "    b_vertical = None,        \n",
    ")\n",
    "\n",
    "# Sensor layout\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "sensors_settings =  gp.SensorsSettings(\n",
    "    layout = None,\n",
    "    sensor_number = jnp.array(7),\n",
    "    measurement_error_var = None,\n",
    "    sensor_seed = None,\n",
    "    measurement_error_seed = None,\n",
    "    sensor_locations =  flatten_list_of_lists([point_sensors_1_location, point_sensors_2_location, point_sensors_3_location, point_sensors_4_location, point_sensors_5_location, point_sensors_6_location, point_sensors_7_location]), \n",
    ")\n",
    "\n",
    "\n",
    "# Gaussian Plume model\n",
    "gaussianplume = gp.GaussianPlume(grid, source_location, wind_field, atmospheric_state, sensors_settings)\n",
    "\n",
    "fixed  = gaussianplume.fixed_objects_of_gridfree_chilbolton_coupling_matrix(simulation = False, wind_direction=wind_field[\"Average Direction\"].values, wind_speed=wind_field[\"Average Speed\"].values, tangamma_ts = tangamma_ts, number_of_time_steps=wind_field.shape[0], source3and4=True, release_17=False)\n",
    "fixed_ref1 = fixed[0], fixed[7], fixed[14], fixed[15], fixed[35], fixed[36], fixed[16], fixed[37], fixed[44]\n",
    "fixed_ref2 = fixed[1], fixed[8], fixed[17], fixed[18], fixed[35], fixed[36], fixed[19], fixed[38], fixed[45]\n",
    "fixed_ref3 = fixed[2], fixed[9], fixed[20], fixed[21], fixed[35], fixed[36], fixed[22], fixed[39], fixed[46]\n",
    "fixed_ref4 = fixed[3], fixed[10], fixed[23], fixed[24], fixed[35], fixed[36], fixed[25], fixed[40], fixed[47]\n",
    "fixed_ref5 = fixed[4], fixed[11], fixed[26], fixed[27], fixed[35], fixed[36], fixed[28], fixed[41], fixed[48]\n",
    "fixed_ref6 = fixed[5], fixed[12], fixed[29], fixed[30], fixed[35], fixed[36], fixed[31], fixed[42], fixed[49]\n",
    "fixed_ref7 = fixed[6], fixed[13], fixed[32], fixed[33], fixed[35], fixed[36], fixed[34], fixed[43], fixed[50]\n",
    "\n",
    "\n",
    "# Parameter priors\n",
    "priors = mcmc.Priors(\n",
    "    # Slab allocation rate prior (used in grid-based inversion)\n",
    "    theta = 0.1,\n",
    "\n",
    "    # Emission rate (log(s)): Log scale Slab and spike prior (used in grid-based inversion)\n",
    "    log_spike_mean = -25.0,\n",
    "    log_spike_var = 10.0,\n",
    "    log_slab_mean = -7.5,\n",
    "    log_slab_var = 1.5,\n",
    "\n",
    "    # Source location (x,y):\n",
    "    source_location_x_mean = 50.0,\n",
    "    source_location_x_var = 25.0,\n",
    "    source_location_y_mean = 50.0,\n",
    "    source_location_y_var = 25.0,\n",
    "\n",
    "    # Measurement error variance (sigma squared)\n",
    "    sigma_squared_con = 1e-11,\n",
    "    sigma_squared_rate = 1e-8,\n",
    "\n",
    "    # Background gas concentration (beta)\n",
    "    mean_background_prior = 1.92,\n",
    "    variance_background_prior = 0.1**2,\n",
    "\n",
    "    # Dispersion parameter (a_H, a_V, b_H, b_V)\n",
    "    a_mean = jnp.log(0.6),\n",
    "    a_var = 0.5**2,\n",
    "    b_mean = jnp.log(0.6),\n",
    "    b_var = 0.2**2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Source 3 and Source 4 Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Grid Search\n",
    "\n",
    "Here we only estimate source emission rate and location while fixing background gas concentration, measurement error variance and dispersion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.        , 0.        ],\n",
       "        [0.00033333, 0.        ],\n",
       "        [0.00066667, 0.        ],\n",
       "        [0.        , 0.00033333],\n",
       "        [0.00033333, 0.00033333],\n",
       "        [0.00066667, 0.00033333],\n",
       "        [0.        , 0.00066667],\n",
       "        [0.00033333, 0.00066667],\n",
       "        [0.00066667, 0.00066667]]),\n",
       " array([[40., 40.],\n",
       "        [50., 40.],\n",
       "        [60., 40.],\n",
       "        [70., 40.],\n",
       "        [80., 40.],\n",
       "        [40., 50.],\n",
       "        [50., 50.],\n",
       "        [60., 50.],\n",
       "        [70., 50.],\n",
       "        [80., 50.],\n",
       "        [40., 60.],\n",
       "        [50., 60.],\n",
       "        [60., 60.],\n",
       "        [70., 60.],\n",
       "        [80., 60.],\n",
       "        [40., 70.],\n",
       "        [50., 70.],\n",
       "        [60., 70.],\n",
       "        [70., 70.],\n",
       "        [80., 70.],\n",
       "        [40., 80.],\n",
       "        [50., 80.],\n",
       "        [60., 80.],\n",
       "        [70., 80.],\n",
       "        [80., 80.]]),\n",
       " array([[  0.,   0.],\n",
       "        [ 10.,   0.],\n",
       "        [ 20.,   0.],\n",
       "        [ 30.,   0.],\n",
       "        [ 40.,   0.],\n",
       "        [ 50.,   0.],\n",
       "        [ 60.,   0.],\n",
       "        [ 70.,   0.],\n",
       "        [ 80.,   0.],\n",
       "        [ 90.,   0.],\n",
       "        [100.,   0.],\n",
       "        [110.,   0.],\n",
       "        [  0.,  10.],\n",
       "        [ 10.,  10.],\n",
       "        [ 20.,  10.],\n",
       "        [ 30.,  10.],\n",
       "        [ 40.,  10.],\n",
       "        [ 50.,  10.],\n",
       "        [ 60.,  10.],\n",
       "        [ 70.,  10.],\n",
       "        [ 80.,  10.],\n",
       "        [ 90.,  10.],\n",
       "        [100.,  10.],\n",
       "        [110.,  10.],\n",
       "        [  0.,  20.],\n",
       "        [ 10.,  20.],\n",
       "        [ 20.,  20.],\n",
       "        [ 30.,  20.],\n",
       "        [ 40.,  20.],\n",
       "        [ 50.,  20.],\n",
       "        [ 60.,  20.],\n",
       "        [ 70.,  20.],\n",
       "        [ 80.,  20.],\n",
       "        [ 90.,  20.],\n",
       "        [100.,  20.],\n",
       "        [110.,  20.],\n",
       "        [  0.,  30.],\n",
       "        [ 10.,  30.],\n",
       "        [ 20.,  30.],\n",
       "        [ 30.,  30.],\n",
       "        [ 40.,  30.],\n",
       "        [ 50.,  30.],\n",
       "        [ 60.,  30.],\n",
       "        [ 70.,  30.],\n",
       "        [ 80.,  30.],\n",
       "        [ 90.,  30.],\n",
       "        [100.,  30.],\n",
       "        [110.,  30.],\n",
       "        [  0.,  40.],\n",
       "        [ 10.,  40.],\n",
       "        [ 20.,  40.],\n",
       "        [ 30.,  40.],\n",
       "        [ 40.,  40.],\n",
       "        [ 50.,  40.],\n",
       "        [ 60.,  40.],\n",
       "        [ 70.,  40.],\n",
       "        [ 80.,  40.],\n",
       "        [ 90.,  40.],\n",
       "        [100.,  40.],\n",
       "        [110.,  40.],\n",
       "        [  0.,  50.],\n",
       "        [ 10.,  50.],\n",
       "        [ 20.,  50.],\n",
       "        [ 30.,  50.],\n",
       "        [ 40.,  50.],\n",
       "        [ 50.,  50.],\n",
       "        [ 60.,  50.],\n",
       "        [ 70.,  50.],\n",
       "        [ 80.,  50.],\n",
       "        [ 90.,  50.],\n",
       "        [100.,  50.],\n",
       "        [110.,  50.],\n",
       "        [  0.,  60.],\n",
       "        [ 10.,  60.],\n",
       "        [ 20.,  60.],\n",
       "        [ 30.,  60.],\n",
       "        [ 40.,  60.],\n",
       "        [ 50.,  60.],\n",
       "        [ 60.,  60.],\n",
       "        [ 70.,  60.],\n",
       "        [ 80.,  60.],\n",
       "        [ 90.,  60.],\n",
       "        [100.,  60.],\n",
       "        [110.,  60.],\n",
       "        [  0.,  70.],\n",
       "        [ 10.,  70.],\n",
       "        [ 20.,  70.],\n",
       "        [ 30.,  70.],\n",
       "        [ 40.,  70.],\n",
       "        [ 50.,  70.],\n",
       "        [ 60.,  70.],\n",
       "        [ 70.,  70.],\n",
       "        [ 80.,  70.],\n",
       "        [ 90.,  70.],\n",
       "        [100.,  70.],\n",
       "        [110.,  70.],\n",
       "        [  0.,  80.],\n",
       "        [ 10.,  80.],\n",
       "        [ 20.,  80.],\n",
       "        [ 30.,  80.],\n",
       "        [ 40.,  80.],\n",
       "        [ 50.,  80.],\n",
       "        [ 60.,  80.],\n",
       "        [ 70.,  80.],\n",
       "        [ 80.,  80.],\n",
       "        [ 90.,  80.],\n",
       "        [100.,  80.],\n",
       "        [110.,  80.],\n",
       "        [  0.,  90.],\n",
       "        [ 10.,  90.],\n",
       "        [ 20.,  90.],\n",
       "        [ 30.,  90.],\n",
       "        [ 40.,  90.],\n",
       "        [ 50.,  90.],\n",
       "        [ 60.,  90.],\n",
       "        [ 70.,  90.],\n",
       "        [ 80.,  90.],\n",
       "        [ 90.,  90.],\n",
       "        [100.,  90.],\n",
       "        [110.,  90.],\n",
       "        [  0., 100.],\n",
       "        [ 10., 100.],\n",
       "        [ 20., 100.],\n",
       "        [ 30., 100.],\n",
       "        [ 40., 100.],\n",
       "        [ 50., 100.],\n",
       "        [ 60., 100.],\n",
       "        [ 70., 100.],\n",
       "        [ 80., 100.],\n",
       "        [ 90., 100.],\n",
       "        [100., 100.],\n",
       "        [110., 100.],\n",
       "        [  0., 110.],\n",
       "        [ 10., 110.],\n",
       "        [ 20., 110.],\n",
       "        [ 30., 110.],\n",
       "        [ 40., 110.],\n",
       "        [ 50., 110.],\n",
       "        [ 60., 110.],\n",
       "        [ 70., 110.],\n",
       "        [ 80., 110.],\n",
       "        [ 90., 110.],\n",
       "        [100., 110.],\n",
       "        [110., 110.]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_granulaty_in_kg_per_s = 0.001/3\n",
    "\n",
    "s_range = np.arange(0, 0.001, emission_granulaty_in_kg_per_s)\n",
    "x_range = grid.x\n",
    "y_range = grid.y\n",
    "\n",
    "def get_ranges_combinations(x, y):\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    combinations = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    return combinations\n",
    "\n",
    "ranges = []\n",
    "ranges.append(get_ranges_combinations(s_range,s_range))\n",
    "ranges.append(get_ranges_combinations(x_range,x_range))\n",
    "ranges.append(get_ranges_combinations(y_range,y_range))\n",
    "\n",
    "ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_likelihood(s, x, y, initial_sgm, initial_betas, scheme, stability_class, number_time_steps, release_17=False):\n",
    "    \"\"\"\n",
    "    Returns the positive log posterior of the point sensors measurements model. \n",
    "\n",
    "    \"\"\"\n",
    "    if stability_class == False:\n",
    "        stability_class = None\n",
    "        estimated = True\n",
    "    else:\n",
    "        estimated = False\n",
    "        \n",
    "    coupling_matrix_ref1 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref1, x, y, None, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref2 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref2, x, y, None, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref3 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref3, x, y, None, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref4 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref4, x, y, None, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref5 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref5, x, y, None, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref6 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref6, x, y, None, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref7 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref7, x, y, None, 1.0, 1.0, 1.0, 1.0, simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    \n",
    "    if release_17:\n",
    "        reshaped_coupling_matrix_ref1_src1 = coupling_matrix_ref1[:,0].reshape(number_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref1_src2 = coupling_matrix_ref1[:,1].reshape(number_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src1 = coupling_matrix_ref2[:,0].reshape(number_time_steps,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src2 = coupling_matrix_ref2[:,1].reshape(number_time_steps,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src1 = coupling_matrix_ref3[:,0].reshape(number_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src2 = coupling_matrix_ref3[:,1].reshape(number_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src1 = coupling_matrix_ref4[:,0].reshape(number_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src2 = coupling_matrix_ref4[:,1].reshape(number_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src1 = coupling_matrix_ref5[:,0].reshape(number_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src2 = coupling_matrix_ref5[:,1].reshape(number_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src1 = coupling_matrix_ref6[:,0].reshape(number_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src2 = coupling_matrix_ref6[:,1].reshape(number_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src1 = coupling_matrix_ref7[:,0].reshape(number_time_steps,17*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src2 = coupling_matrix_ref7[:,1].reshape(number_time_steps,17*5, order='F')\n",
    "    else:\n",
    "        reshaped_coupling_matrix_ref1_src1 = coupling_matrix_ref1[:,0].reshape(number_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref1_src2 = coupling_matrix_ref1[:,1].reshape(number_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src1 = coupling_matrix_ref2[:,0].reshape(number_time_steps,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src2 = coupling_matrix_ref2[:,1].reshape(number_time_steps,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src1 = coupling_matrix_ref3[:,0].reshape(number_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src2 = coupling_matrix_ref3[:,1].reshape(number_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src1 = coupling_matrix_ref4[:,0].reshape(number_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src2 = coupling_matrix_ref4[:,1].reshape(number_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src1 = coupling_matrix_ref5[:,0].reshape(number_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src2 = coupling_matrix_ref5[:,1].reshape(number_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src1 = coupling_matrix_ref6[:,0].reshape(number_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src2 = coupling_matrix_ref6[:,1].reshape(number_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src1 = coupling_matrix_ref7[:,0].reshape(number_time_steps,17*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src2 = coupling_matrix_ref7[:,1].reshape(number_time_steps,17*5, order='F')\n",
    "\n",
    "    path_averaged_coupling_matrix_ref1_src1 = reshaped_coupling_matrix_ref1_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref1_src2 = reshaped_coupling_matrix_ref1_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref2_src1 = reshaped_coupling_matrix_ref2_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref2_src2 = reshaped_coupling_matrix_ref2_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref3_src1 = reshaped_coupling_matrix_ref3_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref3_src2 = reshaped_coupling_matrix_ref3_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref4_src1 = reshaped_coupling_matrix_ref4_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref4_src2 = reshaped_coupling_matrix_ref4_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref5_src1 = reshaped_coupling_matrix_ref5_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref5_src2 = reshaped_coupling_matrix_ref5_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref6_src1 = reshaped_coupling_matrix_ref6_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref6_src2 = reshaped_coupling_matrix_ref6_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref7_src1 = reshaped_coupling_matrix_ref7_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref7_src2 = reshaped_coupling_matrix_ref7_src2.mean(axis=1)\n",
    "    \n",
    "    source_1_path_averaged_A = [path_averaged_coupling_matrix_ref1_src1, path_averaged_coupling_matrix_ref2_src1, path_averaged_coupling_matrix_ref3_src1, path_averaged_coupling_matrix_ref4_src1, path_averaged_coupling_matrix_ref5_src1, path_averaged_coupling_matrix_ref6_src1, path_averaged_coupling_matrix_ref7_src1]\n",
    "    source_2_path_averaged_A = [path_averaged_coupling_matrix_ref1_src2, path_averaged_coupling_matrix_ref2_src2, path_averaged_coupling_matrix_ref3_src2, path_averaged_coupling_matrix_ref4_src2, path_averaged_coupling_matrix_ref5_src2, path_averaged_coupling_matrix_ref6_src2, path_averaged_coupling_matrix_ref7_src2]\n",
    "    \n",
    "    if release_17 == False:\n",
    "        source_1_A = jnp.array(source_1_path_averaged_A).reshape(-1,1)\n",
    "        source_2_A = jnp.array(source_2_path_averaged_A).reshape(-1,1)\n",
    "        A = jnp.concatenate([source_1_A, source_2_A], axis=1)\n",
    "        log_likelihood = tfd.Normal(loc = (jnp.matmul(A,s.reshape(-1,1)) + jnp.repeat(initial_betas, number_time_steps).reshape(-1,1)), \\\n",
    "                                    scale= jnp.sqrt(initial_sgm)).log_prob(data)\n",
    "        log_posterior = jnp.sum(log_likelihood)\n",
    "    else:\n",
    "        source_1_A = jnp.array([item for sublist in source_1_path_averaged_A for item in sublist]).reshape((number_time_steps*6)+number_time_steps,2)\n",
    "        source_2_A = jnp.array([item for sublist in source_2_path_averaged_A for item in sublist]).reshape((number_time_steps*6)+number_time_steps,2)\n",
    "        A = jnp.concatenate([source_1_A, source_2_A], axis=1)\n",
    "        log_likelihood = tfd.Normal(loc = (jnp.matmul(A,s.reshape(-1,1)) + jnp.repeat(initial_betas.mean(), (number_time_steps*6)+number_time_steps).reshape(-1,1)), \\\n",
    "                                    scale= jnp.sqrt(initial_sgm)).log_prob(data)\n",
    "\n",
    "    return log_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter combinations: 32400\n"
     ]
    }
   ],
   "source": [
    "# Generate all combinations of parameters\n",
    "sources_param = jnp.zeros(len(ranges)*2)\n",
    "parameter_combinations = jnp.array(list(itertools.product(*ranges[:len(ranges)+1])))\n",
    "print(f\"Number of parameter combinations: {parameter_combinations.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search maximum likelihood estimation of emission rate and location:\n",
      "             s1        s2    x1    x2    y1     y2\n",
      "21580  0.000667  0.000333  80.0  80.0  40.0  100.0\n"
     ]
    }
   ],
   "source": [
    "# Initial parameter values for:\n",
    "\n",
    "# Sensor measurement error variance\n",
    "initial_sgm = 1e-5\n",
    "# Background gas concentration\n",
    "initial_betas = jnp.repeat(1.92, 7)\n",
    "\n",
    "# Step function\n",
    "def one_step(_, parameters):\n",
    "    s, x, y = parameters[0], parameters[1], parameters[2]\n",
    "    new_likelihood = search_likelihood(s, x, y, initial_sgm, initial_betas, \"Draxler\", False, wind_field.shape[0], release_17=False)\n",
    "    return new_likelihood, new_likelihood\n",
    "# Use lax.scan to iterate over the parameter combinations\n",
    "_, likelihoods = lax.scan(one_step, 0.0, parameter_combinations)\n",
    "# Analysing output\n",
    "likelihood_df = pd.DataFrame(likelihoods, columns = ['log_likelihood'])\n",
    "parameters_df = pd.DataFrame(parameter_combinations.reshape(-1,6), columns = [\"s1\", \"s2\", \"x1\", \"x2\", \"y1\", \"y2\"])\n",
    "max_likelihood_index = likelihood_df.idxmax()\n",
    "max_likelihood_row = parameters_df.iloc[max_likelihood_index]\n",
    "sources_param = sources_param.at[:].set(max_likelihood_row.values.flatten())\n",
    "print(f\"Grid search maximum likelihood estimation of emission rate and location:\")\n",
    "print(max_likelihood_row)\n",
    "\n",
    "rate_1 = max_likelihood_row.values.flatten()[0]\n",
    "rate_2 = max_likelihood_row.values.flatten()[1]\n",
    "loc_x_1 = max_likelihood_row.values.flatten()[2]\n",
    "loc_x_2 = max_likelihood_row.values.flatten()[3]\n",
    "loc_y_1 = max_likelihood_row.values.flatten()[4]\n",
    "loc_y_2 = max_likelihood_row.values.flatten()[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Latin Hypercube\n",
    "\n",
    "Here we estimate all parameters simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latin Hypercube maximum likelihood estimation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_1</th>\n",
       "      <th>s_2</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>sgm</th>\n",
       "      <th>a_h</th>\n",
       "      <th>a_v</th>\n",
       "      <th>b_h</th>\n",
       "      <th>b_v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10492</th>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>74.328962</td>\n",
       "      <td>70.322401</td>\n",
       "      <td>44.000383</td>\n",
       "      <td>100.905596</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>1.138209</td>\n",
       "      <td>1.141391</td>\n",
       "      <td>0.959233</td>\n",
       "      <td>0.907669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            s_1       s_2        x_1        x_2        y_1         y_2  \\\n",
       "10492  0.000544  0.000154  74.328962  70.322401  44.000383  100.905596   \n",
       "\n",
       "            sgm       a_h       a_v       b_h       b_v  \n",
       "10492  0.000095  1.138209  1.141391  0.959233  0.907669  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def latin_hypercube_sampling(param_ranges, num_samples):\n",
    "    \"\"\"\n",
    "    Generate a Latin Hypercube Sample within specified parameter ranges.\n",
    "\n",
    "    Parameters:\n",
    "    param_ranges (list of tuple): A list of tuples specifying the range (min, max) for each parameter.\n",
    "    num_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A num_samples x num_params matrix where each column corresponds to a parameter and each row corresponds to a sample.\n",
    "    \"\"\"\n",
    "    num_params = len(param_ranges)\n",
    "    \n",
    "    # Generate the Latin Hypercube Sample\n",
    "    lhd = lhs(num_params, num_samples)\n",
    "    \n",
    "    # Scale the samples to be within the parameter ranges\n",
    "    for i in range(num_params):\n",
    "        min_val, max_val = param_ranges[i]\n",
    "        lhd[:, i] = lhd[:, i] * (max_val - min_val) + min_val\n",
    "\n",
    "    return lhd\n",
    "\n",
    "\n",
    "def latin_hypercube_likelihood(s1, s2, x1, x2, y1, y2, sgm, a_h, a_v, b_h, b_v, scheme, stability_class, number_time_steps, release_17=False):\n",
    "    \"\"\"\n",
    "    Returns the positive log posterior of the point sensors measurements model. \n",
    "\n",
    "    \"\"\"\n",
    "    if stability_class == False:\n",
    "        stability_class = None\n",
    "        estimated = True\n",
    "    else:\n",
    "        estimated = False\n",
    "\n",
    "    coupling_matrix_ref1 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref1, [x1,x2], [y1,y2], None, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref2 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref2, [x1,x2], [y1,y2], None, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref3 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref3, [x1,x2], [y1,y2], None, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref4 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref4, [x1,x2], [y1,y2], None, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref5 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref5, [x1,x2], [y1,y2], None, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref6 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref6, [x1,x2], [y1,y2], None, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    coupling_matrix_ref7 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref7, [x1,x2], [y1,y2], None, jnp.array(a_h), jnp.array(a_v), jnp.array(b_h), jnp.array(b_v), simulation=False, estimated=estimated, scheme=scheme, stability_class=stability_class)\n",
    "    \n",
    "    if release_17:\n",
    "        reshaped_coupling_matrix_ref1_src1 = coupling_matrix_ref1[:,0].reshape(number_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref1_src2 = coupling_matrix_ref1[:,1].reshape(number_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src1 = coupling_matrix_ref2[:,0].reshape(number_time_steps - 31,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src2 = coupling_matrix_ref2[:,1].reshape(number_time_steps - 31,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src1 = coupling_matrix_ref3[:,0].reshape(number_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src2 = coupling_matrix_ref3[:,1].reshape(number_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src1 = coupling_matrix_ref4[:,0].reshape(number_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src2 = coupling_matrix_ref4[:,1].reshape(number_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src1 = coupling_matrix_ref5[:,0].reshape(number_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src2 = coupling_matrix_ref5[:,1].reshape(number_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src1 = coupling_matrix_ref6[:,0].reshape(number_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src2 = coupling_matrix_ref6[:,1].reshape(number_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src1 = coupling_matrix_ref7[:,0].reshape(number_time_steps,17*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src2 = coupling_matrix_ref7[:,1].reshape(number_time_steps,17*5, order='F')\n",
    "    else:\n",
    "        reshaped_coupling_matrix_ref1_src1 = coupling_matrix_ref1[:,0].reshape(number_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref1_src2 = coupling_matrix_ref1[:,1].reshape(number_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src1 = coupling_matrix_ref2[:,0].reshape(number_time_steps,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src2 = coupling_matrix_ref2[:,1].reshape(number_time_steps,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src1 = coupling_matrix_ref3[:,0].reshape(number_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src2 = coupling_matrix_ref3[:,1].reshape(number_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src1 = coupling_matrix_ref4[:,0].reshape(number_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src2 = coupling_matrix_ref4[:,1].reshape(number_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src1 = coupling_matrix_ref5[:,0].reshape(number_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src2 = coupling_matrix_ref5[:,1].reshape(number_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src1 = coupling_matrix_ref6[:,0].reshape(number_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src2 = coupling_matrix_ref6[:,1].reshape(number_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src1 = coupling_matrix_ref7[:,0].reshape(number_time_steps,17*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src2 = coupling_matrix_ref7[:,1].reshape(number_time_steps,17*5, order='F')\n",
    "    \n",
    "    path_averaged_coupling_matrix_ref1_src1 = reshaped_coupling_matrix_ref1_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref1_src2 = reshaped_coupling_matrix_ref1_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref2_src1 = reshaped_coupling_matrix_ref2_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref2_src2 = reshaped_coupling_matrix_ref2_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref3_src1 = reshaped_coupling_matrix_ref3_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref3_src2 = reshaped_coupling_matrix_ref3_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref4_src1 = reshaped_coupling_matrix_ref4_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref4_src2 = reshaped_coupling_matrix_ref4_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref5_src1 = reshaped_coupling_matrix_ref5_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref5_src2 = reshaped_coupling_matrix_ref5_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref6_src1 = reshaped_coupling_matrix_ref6_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref6_src2 = reshaped_coupling_matrix_ref6_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref7_src1 = reshaped_coupling_matrix_ref7_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref7_src2 = reshaped_coupling_matrix_ref7_src2.mean(axis=1)\n",
    "\n",
    "    source_1_path_averaged_A = [path_averaged_coupling_matrix_ref1_src1, path_averaged_coupling_matrix_ref2_src1, path_averaged_coupling_matrix_ref3_src1, path_averaged_coupling_matrix_ref4_src1, path_averaged_coupling_matrix_ref5_src1, path_averaged_coupling_matrix_ref6_src1, path_averaged_coupling_matrix_ref7_src1]\n",
    "    source_2_path_averaged_A = [path_averaged_coupling_matrix_ref1_src2, path_averaged_coupling_matrix_ref2_src2, path_averaged_coupling_matrix_ref3_src2, path_averaged_coupling_matrix_ref4_src2, path_averaged_coupling_matrix_ref5_src2, path_averaged_coupling_matrix_ref6_src2, path_averaged_coupling_matrix_ref7_src2]\n",
    "    \n",
    "    if release_17 == False:\n",
    "        source_1_A = jnp.array(source_1_path_averaged_A).reshape(-1,1)\n",
    "        source_2_A = jnp.array(source_2_path_averaged_A).reshape(-1,1)\n",
    "        A = jnp.concatenate([source_1_A, source_2_A], axis=1)\n",
    "        log_likelihood = tfd.Normal(loc = (jnp.matmul(A,jnp.array([s1,s2]).reshape(-1,1)) + jnp.repeat(initial_betas, number_time_steps).reshape(-1,1)), \\\n",
    "                                    scale= jnp.sqrt(sgm)).log_prob(data)\n",
    "        log_posterior = jnp.sum(log_likelihood)\n",
    "    else:\n",
    "        source_1_A = jnp.array([item for sublist in source_1_path_averaged_A for item in sublist]).reshape((number_time_steps*6)+number_time_steps,2)\n",
    "        source_2_A = jnp.array([item for sublist in source_2_path_averaged_A for item in sublist]).reshape((number_time_steps*6)+number_time_steps,2)\n",
    "        A = jnp.concatenate([source_1_A, source_2_A], axis=1)\n",
    "        log_likelihood = tfd.Normal(loc = (jnp.matmul(A,jnp.array([s1,s2]).reshape(-1,1)) + jnp.repeat(initial_betas.mean(), (number_time_steps*6)+number_time_steps).reshape(-1,1)), \\\n",
    "                                    scale= jnp.sqrt(sgm)).log_prob(data)\n",
    "\n",
    "    return log_posterior\n",
    "\n",
    "param_ranges = []\n",
    "param_ranges.append((np.maximum(0, rate_1 - emission_granulaty_in_kg_per_s), rate_1 + emission_granulaty_in_kg_per_s))\n",
    "param_ranges.append((np.maximum(0, rate_2 - emission_granulaty_in_kg_per_s), rate_2 + emission_granulaty_in_kg_per_s))\n",
    "param_ranges.append((loc_x_1 - grid.dx, loc_x_1 + grid.dx))\n",
    "param_ranges.append((loc_x_2 - grid.dx, loc_x_2 + grid.dx))\n",
    "param_ranges.append((loc_y_1 - grid.dy, loc_y_1 + grid.dy))\n",
    "param_ranges.append((loc_y_2 - grid.dy, loc_y_2 + grid.dy))\n",
    "param_ranges.append((0, 1e-4))\n",
    "param_ranges.append((0.5, 1.2))\n",
    "param_ranges.append((0.5, 1.2))\n",
    "param_ranges.append((0.5, 1.01))\n",
    "param_ranges.append((0.5, 1.01))\n",
    "\n",
    "\n",
    "num_samples = 50_000\n",
    "lh_samples = latin_hypercube_sampling(param_ranges, num_samples)\n",
    "# Step function\n",
    "def lh_one_step(_, parameters):\n",
    "    rate_1, rate_2, loc_x_1, loc_x_2, loc_y_1, loc_y_2, sgm, a_h, a_v, b_h, b_v = parameters\n",
    "    new_likelihood = latin_hypercube_likelihood(rate_1, rate_2, loc_x_1, loc_x_2, loc_y_1, loc_y_2, sgm, a_h, a_v, b_h, b_v, \"Draxler\", False, wind_field.shape[0])\n",
    "    return new_likelihood, new_likelihood\n",
    "# Use lax.scan to iterate over the parameter combinations\n",
    "_, lh_likelihoods = lax.scan(lh_one_step, 0.0, lh_samples)\n",
    "\n",
    "# Analysing output\n",
    "lh_likelihood_df = pd.DataFrame(lh_likelihoods, columns = ['log_likelihood'])\n",
    "lh_parameters_df = pd.DataFrame(lh_samples, columns = ['s_1', 's_2', 'x_1', 'x_2', 'y_1', 'y_2', 'sgm', 'a_h', 'a_v', 'b_h', 'b_v'])\n",
    "lh_max_likelihood_index = lh_likelihood_df.idxmax()\n",
    "lh_max_likelihood_row = lh_parameters_df.iloc[lh_max_likelihood_index]\n",
    "print(f\"Latin Hypercube maximum likelihood estimation:\")\n",
    "lh_max_likelihood_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Manifold-Metropolis-within-Gibbs\n",
    "\n",
    "Here we estimate all parameters simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Initial Parameter Values:\n",
      "--------------------------------\n",
      "Initial rates: [-7.51653702 -8.77812005]\n",
      "Initial locations x: [74.32896237 70.32240054]\n",
      "Initial locations y: [ 44.00038264 100.90559624]\n",
      "Initial sgm sqr: 9.534536963112652e-05\n",
      "Initial a_h: 0.12945601579621763\n",
      "Initial a_v: 0.1322478785039079\n",
      "Initial b_h: -0.04162160332308528\n",
      "Initial b_v: -0.0968756455617121\n"
     ]
    }
   ],
   "source": [
    "initial_log_rates = jnp.log(lh_max_likelihood_row[['s_' + str(i+1) for i in range(int(len(ranges)/3)*2)]].values.flatten())\n",
    "initial_locations_x = lh_max_likelihood_row[['x_' + str(i+1) for i in range(int(len(ranges)/3)*2)]].values.flatten()\n",
    "initial_locations_y = lh_max_likelihood_row[['y_' + str(i+1) for i in range(int(len(ranges)/3)*2)]].values.flatten()\n",
    "initial_sgm_sqr = lh_max_likelihood_row.values.flatten()[6]\n",
    "initial_log_a_h = jnp.log(lh_max_likelihood_row.values.flatten()[7])\n",
    "initial_log_a_v = jnp.log(lh_max_likelihood_row.values.flatten()[8])\n",
    "initial_log_b_h = jnp.log(lh_max_likelihood_row.values.flatten()[9])\n",
    "initial_log_b_v = jnp.log(lh_max_likelihood_row.values.flatten()[10])\n",
    "\n",
    "\n",
    "print(f\"Setting Initial Parameter Values:\")\n",
    "print(f\"--------------------------------\")\n",
    "print(f\"Initial rates: {initial_log_rates}\")\n",
    "print(f\"Initial locations x: {initial_locations_x}\")\n",
    "print(f\"Initial locations y: {initial_locations_y}\")\n",
    "print(f\"Initial sgm sqr: {initial_sgm_sqr}\")\n",
    "print(f\"Initial a_h: {initial_log_a_h}\")\n",
    "print(f\"Initial a_v: {initial_log_a_v}\")\n",
    "print(f\"Initial b_h: {initial_log_b_h}\")\n",
    "print(f\"Initial b_v: {initial_log_b_v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gibbsparams = {\n",
    "    'background': initial_betas,\n",
    "    'sigma_squared':   initial_sgm_sqr,\n",
    "}\n",
    "gibbs_flat, gibbs_unflat_func = ravel_pytree(Gibbsparams)\n",
    "\n",
    "MHparams = {\n",
    "    'log_a_H': initial_log_a_h,\n",
    "    'log_a_V': initial_log_a_v,\n",
    "    'log_b_H':  initial_log_b_h,\n",
    "    'log_b_V': initial_log_b_v,\n",
    "    'log_s': jnp.array(initial_log_rates),\n",
    "    'source_x': jnp.array(initial_locations_x),\n",
    "    'source_y': jnp.array(initial_locations_y),\n",
    "}\n",
    "mh_flat, mh_unflat_func = ravel_pytree(MHparams)\n",
    "\n",
    "\n",
    "def log_posterior(params, sigma_squared, betas, ss_var, ss_mean, data, priors, wind_sigma, number_of_time_steps, release_17=False):\n",
    "    \"\"\"\n",
    "    Returns the positive log posterior of the point sensors measurements model. \n",
    "\n",
    "    \"\"\"\n",
    "    if wind_sigma == True:\n",
    "        coupling_matrix_ref1 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref1, params[\"source_x\"], params[\"source_y\"], None, jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref2 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref2, params[\"source_x\"], params[\"source_y\"], None, jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref3 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref3, params[\"source_x\"], params[\"source_y\"], None, jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref4 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref4, params[\"source_x\"], params[\"source_y\"], None, jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref5 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref5, params[\"source_x\"], params[\"source_y\"], None, jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref6 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref6, params[\"source_x\"], params[\"source_y\"], None, jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref7 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref7, params[\"source_x\"], params[\"source_y\"], None, jnp.exp(params[\"log_a_H\"]), jnp.exp(params[\"log_a_V\"]), jnp.exp(params[\"log_b_H\"]), jnp.exp(params[\"log_b_V\"]), simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "    elif wind_sigma == False:\n",
    "        coupling_matrix_ref1 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref1,params[\"source_x\"],params[\"source_y\"], None, False, False, False, False, simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref2 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref2,params[\"source_x\"],params[\"source_y\"], None, False, False, False, False, simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref3 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref3,params[\"source_x\"],params[\"source_y\"], None, False, False, False, False, simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref4 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref4,params[\"source_x\"],params[\"source_y\"], None, False, False, False, False, simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref5 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref5,params[\"source_x\"],params[\"source_y\"], None, False, False, False, False, simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref6 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref6,params[\"source_x\"],params[\"source_y\"], None, False, False, False, False, simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "        coupling_matrix_ref7 = gaussianplume.temporal_gridfree_coupling_matrix(fixed_ref7,params[\"source_x\"],params[\"source_y\"], None, False, False, False, False, simulation=False, estimated=True, scheme=\"Draxler\", stability_class=\"D\")\n",
    "    if release_17:\n",
    "        reshaped_coupling_matrix_ref1_src1 = coupling_matrix_ref1[:,0].reshape(number_of_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref1_src2 = coupling_matrix_ref1[:,1].reshape(number_of_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src1 = coupling_matrix_ref2[:,0].reshape(number_of_time_steps - 31,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src2 = coupling_matrix_ref2[:,1].reshape(number_of_time_steps - 31,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src1 = coupling_matrix_ref3[:,0].reshape(number_of_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src2 = coupling_matrix_ref3[:,1].reshape(number_of_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src1 = coupling_matrix_ref4[:,0].reshape(number_of_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src2 = coupling_matrix_ref4[:,1].reshape(number_of_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src1 = coupling_matrix_ref5[:,0].reshape(number_of_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src2 = coupling_matrix_ref5[:,1].reshape(number_of_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src1 = coupling_matrix_ref6[:,0].reshape(number_of_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src2 = coupling_matrix_ref6[:,1].reshape(number_of_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src1 = coupling_matrix_ref7[:,0].reshape(number_of_time_steps,17*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src2 = coupling_matrix_ref7[:,1].reshape(number_of_time_steps,17*5, order='F')\n",
    "    else:\n",
    "        reshaped_coupling_matrix_ref1_src1 = coupling_matrix_ref1[:,0].reshape(number_of_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref1_src2 = coupling_matrix_ref1[:,1].reshape(number_of_time_steps,18*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src1 = coupling_matrix_ref2[:,0].reshape(number_of_time_steps,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref2_src2 = coupling_matrix_ref2[:,1].reshape(number_of_time_steps,33*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src1 = coupling_matrix_ref3[:,0].reshape(number_of_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref3_src2 = coupling_matrix_ref3[:,1].reshape(number_of_time_steps,22*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src1 = coupling_matrix_ref4[:,0].reshape(number_of_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref4_src2 = coupling_matrix_ref4[:,1].reshape(number_of_time_steps,49*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src1 = coupling_matrix_ref5[:,0].reshape(number_of_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref5_src2 = coupling_matrix_ref5[:,1].reshape(number_of_time_steps,42*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src1 = coupling_matrix_ref6[:,0].reshape(number_of_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref6_src2 = coupling_matrix_ref6[:,1].reshape(number_of_time_steps,29*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src1 = coupling_matrix_ref7[:,0].reshape(number_of_time_steps,17*5, order='F')\n",
    "        reshaped_coupling_matrix_ref7_src2 = coupling_matrix_ref7[:,1].reshape(number_of_time_steps,17*5, order='F')\n",
    "\n",
    "    path_averaged_coupling_matrix_ref1_src1 = reshaped_coupling_matrix_ref1_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref1_src2 = reshaped_coupling_matrix_ref1_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref2_src1 = reshaped_coupling_matrix_ref2_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref2_src2 = reshaped_coupling_matrix_ref2_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref3_src1 = reshaped_coupling_matrix_ref3_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref3_src2 = reshaped_coupling_matrix_ref3_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref4_src1 = reshaped_coupling_matrix_ref4_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref4_src2 = reshaped_coupling_matrix_ref4_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref5_src1 = reshaped_coupling_matrix_ref5_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref5_src2 = reshaped_coupling_matrix_ref5_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref6_src1 = reshaped_coupling_matrix_ref6_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref6_src2 = reshaped_coupling_matrix_ref6_src2.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref7_src1 = reshaped_coupling_matrix_ref7_src1.mean(axis=1)\n",
    "    path_averaged_coupling_matrix_ref7_src2 = reshaped_coupling_matrix_ref7_src2.mean(axis=1)\n",
    "    \n",
    "    source_1_path_averaged_A = [path_averaged_coupling_matrix_ref1_src1, path_averaged_coupling_matrix_ref2_src1, path_averaged_coupling_matrix_ref3_src1, path_averaged_coupling_matrix_ref4_src1, path_averaged_coupling_matrix_ref5_src1, path_averaged_coupling_matrix_ref6_src1, path_averaged_coupling_matrix_ref7_src1]\n",
    "    source_2_path_averaged_A = [path_averaged_coupling_matrix_ref1_src2, path_averaged_coupling_matrix_ref2_src2, path_averaged_coupling_matrix_ref3_src2, path_averaged_coupling_matrix_ref4_src2, path_averaged_coupling_matrix_ref5_src2, path_averaged_coupling_matrix_ref6_src2, path_averaged_coupling_matrix_ref7_src2]\n",
    "    source_1_A = jnp.array(source_1_path_averaged_A).reshape(-1,1)\n",
    "    source_2_A = jnp.array(source_2_path_averaged_A).reshape(-1,1)\n",
    "\n",
    "    A = jnp.concatenate([source_1_A, source_2_A], axis=1)\n",
    "\n",
    "    log_likelihood = tfd.Normal(loc = (jnp.matmul(A,jnp.exp(params[\"log_s\"]).reshape(-1,1))+ betas), \\\n",
    "                                scale= jnp.sqrt(sigma_squared)).log_prob(data)\n",
    "\n",
    "    if wind_sigma == True:\n",
    "        log_prior_a_H = tfd.Normal(loc = priors.a_mean, scale = jnp.sqrt(priors.a_var)).log_prob(params[\"log_a_H\"])\n",
    "        log_prior_a_V = tfd.Normal(loc = priors.a_mean, scale = jnp.sqrt(priors.a_var)).log_prob(params[\"log_a_V\"])\n",
    "        log_prior_b_H = tfd.Normal(loc = priors.b_mean, scale = jnp.sqrt(priors.b_var)).log_prob(params[\"log_b_H\"])\n",
    "        log_prior_b_V = tfd.Normal(loc = priors.b_mean, scale = jnp.sqrt(priors.b_var)).log_prob(params[\"log_b_V\"])\n",
    "\n",
    "    log_posterior_emission_rate = tfd.MultivariateNormalDiag(loc = ss_mean, scale_diag = jnp.sqrt(ss_var)).log_prob(params[\"log_s\"].reshape(-1,1))\n",
    "    log_posterior_source_location = tfd.MultivariateNormalDiag(loc = jnp.array([priors.source_location_x_mean, priors.source_location_x_mean, priors.source_location_y_mean, priors.source_location_y_mean]), \\\n",
    "                                                                scale_diag = jnp.sqrt(jnp.array([priors.source_location_x_var, priors.source_location_x_var, priors.source_location_y_var, priors.source_location_y_var]))).log_prob(jnp.array([params[\"source_x\"], params[\"source_y\"]]).flatten())\n",
    "\n",
    "    if wind_sigma == True:\n",
    "        log_posterior = jnp.sum(log_likelihood) + jnp.sum(log_posterior_source_location) + jnp.sum(log_posterior_emission_rate) + jnp.sum(log_prior_a_H) + jnp.sum(log_prior_a_V) + jnp.sum(log_prior_b_H) + jnp.sum(log_prior_b_V) \n",
    "    elif wind_sigma == False:\n",
    "        log_posterior = jnp.sum(log_likelihood) + jnp.sum(log_posterior_source_location) + jnp.sum(log_posterior_emission_rate)\n",
    "\n",
    "    return log_posterior, A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 14:23:02.374332: E external/xla/xla/service/slow_operation_alarm.cc:65] \n",
      "********************************\n",
      "[Compiling module jit_scan] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "2024-07-23 14:23:51.100855: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 2m48.726577964s\n",
      "\n",
      "********************************\n",
      "[Compiling module jit_scan] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time Manifold MALA within Gibbs: 1631 minutes 11 seconds\n"
     ]
    }
   ],
   "source": [
    "iterations = 50_000\n",
    "initial_step_size = 0.01\n",
    "# Run the MCMC algorithm\n",
    "mala_chains = mcmc.Manifold_MALA_Within_Gibbs(False, gaussianplume, data, log_posterior, priors, MHparams, Gibbsparams, fixed, chilbolton=True, wind_sigmas=True, release_17 = False, step_size_tuning=\"False\").manifold_mala_chains(Gibbsparams, mh_flat, iterations, initial_step_size, release_17=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
